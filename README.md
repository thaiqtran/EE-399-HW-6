# Thai Tran EE-399-HW-6
## Abstract
This project focuses on analyzing the performance of an LSTM/decoder model for sea-surface temperature (SST) data. The project investigates the model's performance in relation to three key factors: the time lag variable, noise levels, and the number of sensors.The initial steps involve downloading the example code and data provided, which includes the necessary libraries and functions for data processing and model training. The LSTM/decoder model, implemented using the SHRED architecture, is trained on the SST dataset. The trained model is then evaluated to assess its performance in terms of reconstructing the SST data. To analyze the impact of the time lag variable, the performance of the model is examined for different time lag values. The reconstruction error is calculated, and a performance plot is generated, showcasing the relationship between the time lag and the model's accuracy. Next, the influence of noise on the model's performance is investigated. Gaussian noise is added to the SST data, and the model is trained and evaluated with varying levels of noise. The reconstruction error is quantified, and a performance plot is presented to visualize the relationship between noise levels and model accuracy. Furthermore, the impact of the number of sensors on the model's performance is explored. The model is trained and tested with different sensor counts, randomly selecting sensor locations. The reconstruction error is measured, and a performance plot is created, demonstrating the effect of sensor count on the model's accuracy. Through these analyses, valuable insights into the behavior and performance of the LSTM/decoder model for SST data are obtained. The results provide valuable information for understanding the optimal time lag, noise tolerance, and sensor count configurations for the model. The findings contribute to the broader understanding of LSTM-based approaches for time series data analysis and can guide future research in related domains.
## Section I Introduction and Overview
This project focuses on exploring the performance of an LSTM/decoder model for sea-surface temperature (SST) data analysis. SST data plays a crucial role in understanding and predicting various oceanic phenomena, making accurate analysis essential for climate research, weather forecasting, and environmental monitoring. The LSTM/decoder model employed in this project offers a powerful approach to capture temporal dependencies and reconstruct missing or noisy SST data. The main objective of this project is to evaluate the performance of the LSTM/decoder model under different scenarios. Specifically, three key analyses are conducted. First, the performance is assessed as a function of the time lag variable, exploring how varying the length of historical information impacts the model's ability to reconstruct and forecast SST data accurately. Second, the model's resilience to noise is examined by introducing Gaussian noise to the input data and evaluating its impact on the reconstruction quality. Lastly, the influence of the number of sensors on the model's performance is investigated, shedding light on the trade-off between data availability and reconstruction accuracy. By conducting these analyses, we gain insights into the strengths and limitations of the LSTM/decoder model for SST data analysis. The findings contribute to advancing our understanding of deep learning techniques in climate and oceanographic research. Additionally, the results can guide future applications of LSTM-based models in environmental monitoring, climate modeling, and data-driven decision-making processes.
## Section II Theoretical Background
The theoretical foundation of this project lies in the utilization of LSTM/decoder models for sea-surface temperature (SST) data analysis. SST data, which represents the temperature of the upper layer of the ocean, is a critical component in understanding climate patterns, oceanic processes, and their influence on global climate dynamics. Analyzing SST data requires capturing temporal dependencies and spatial correlations, making LSTM/decoder models an ideal choice. Long Short-Term Memory (LSTM) is a variant of recurrent neural networks (RNNs) that can effectively model and learn from sequential data. Unlike traditional RNNs, LSTMs are specifically designed to alleviate the vanishing gradient problem, which can hinder the learning process in deep networks. LSTMs achieve this through a memory cell mechanism that allows the network to selectively retain or forget information over time, enabling the capture of long-range dependencies in the data. The LSTM/decoder architecture comprises two main components: an encoder and a decoder. The encoder processes the input sequence, which in the case of SST data corresponds to historical temperature measurements, and transforms it into a fixed-length representation called the context vector. The context vector serves as a condensed and informative representation of the input sequence, capturing relevant temporal information. This encoding step allows the model to effectively capture and encode the temporal dynamics of the SST data. The decoder component takes the context vector as input and generates reconstructed or forecasted SST data. It learns to map the encoded representation back into the original data space, effectively reconstructing the SST field. The decoder can also be used for predicting future SST values by conditioning the model on the available historical data. This forecasting capability is crucial for understanding and predicting climate patterns and phenomena, aiding in climate modeling, weather forecasting, and environmental monitoring. To train the LSTM/decoder model, a loss function is typically employed to quantify the discrepancy between the model's output and the ground truth SST data. Mean squared error (MSE) is commonly used as the loss function, measuring the average squared difference between the predicted and actual SST values. Through the iterative process of optimizing this loss function using gradient-based optimization algorithms such as stochastic gradient descent (SGD), the LSTM/decoder learns to extract meaningful features from the input sequence and generate accurate reconstructions or predictions. The theoretical understanding of LSTM/decoder models provides the necessary foundation for evaluating their performance in different scenarios. By leveraging the capacity of LSTMs to capture temporal dependencies, we can assess the impact of varying time lags, noise levels, and the number of sensors on the model's ability to reconstruct SST data accurately. These analyses contribute to advancing the application of deep learning techniques in climate research, enabling us to gain deeper insights into oceanic processes, climate dynamics, and their interconnections. Ultimately, this work holds the potential to enhance our understanding of the Earth's climate system and its response to environmental changes.
## Sec. III. Algorithm Implementation and Development
The implementation of the LSTM/decoder algorithm for SST data analysis involved several key steps. The following outlines the algorithm's development and implementation process using the provided code snippets:
1) Data Loading and Scaling: The SST data was loaded using the load_data function, and the data was scaled using the MinMaxScaler from scikit-learn. The scaled data, stored in the transformed_X variable, was used as the input for the algorithm.
2) Model Architecture: The SHRED model, which combines an encoder and a decoder, was implemented. The encoder part of the model used LSTM layers to process the input sequence and capture the temporal information. The decoder part of the model also utilized LSTM layers to reconstruct or predict the SST data based on the learned context.
3) Training and Validation: The SHRED model was trained and validated using the training and validation datasets. The training data and ground truth values were transformed into PyTorch tensors (train_data_in, train_data_out) and used to create the train_dataset. Similarly, the validation data was transformed and used to create the valid_dataset. The model was then trained using the fit function, which optimized the model parameters based on the training data and monitored the validation error to prevent overfitting.
4) Performance Evaluation: The trained SHRED model was evaluated on a test dataset to assess its performance. The test data was transformed into PyTorch tensors (test_data_in, test_data_out) and used to create the test_dataset. The model's predictions (test_recons) were obtained by passing the test input data through the SHRED model and then inverse transformed using the inverse_transform method of the MinMaxScaler. The reconstruction error, calculated as the normalized difference between the predicted and ground truth values, was used as the performance metric for the algorithm.
By following these steps, the LSTM/decoder algorithm was successfully implemented and applied to analyze the SST data. The algorithm demonstrated the ability to capture temporal dependencies and accurately reconstruct or predict the SST values, as reflected in the performance evaluation results.

## Sec. IV. Computational Results
The implemented LSTM/decoder algorithm for SST data analysis yielded promising computational results. The algorithm was evaluated and analyzed in three aspects: performance as a function of the time lag variable, performance as a function of noise levels, and performance as a function of the number of sensors. But first, we needed to make sure that the give sample code was correct shown in figure blow
![image](https://github.com/thaiqtran/EE-399-HW-6/assets/129792715/f873bffd-153b-41a1-b6d6-9537879472e1)

### Performance as a Function of Time Lag
To investigate the impact of the time lag variable on the algorithm's performance, different time lag values were evaluated. The reconstruction error was calculated for each time lag value, and the results were plotted in the figure below. The plot demonstrated the relationship between the time lag and the reconstruction error. It provided insights into the optimal time lag value for capturing the temporal dependencies in the SST data. 
![image](https://github.com/thaiqtran/EE-399-HW-6/assets/129792715/25dd39da-a42c-42b2-af0f-adc1ee78d88a)
### Performance as a Function of Noise Levels
The algorithm's robustness to noise was assessed by introducing Gaussian noise to the scaled SST data at different levels. The algorithm was trained and tested on the noisy data, and the reconstruction error was computed for each noise level. The results shown below indicated the algorithm's ability to handle varying levels of noise and provided insights into its resilience and stability in real-world scenarios.
![image](https://github.com/thaiqtran/EE-399-HW-6/assets/129792715/8ab3035f-6794-4506-a076-a2b930a91c55)
### Performance as a Function of the Number of Sensors
The algorithm's performance was analyzed with respect to the number of sensors used for SST data analysis. By varying the number of sensors, the algorithm's reconstruction accuracy was evaluated. The results demonstrated the relationship between the number of sensors and the reconstruction error, shedding light on the optimal sensor configuration for accurate SST analysis. These computational results, shown below, showcase the effectiveness and versatility of the implemented LSTM/decoder algorithm in analyzing SST data. The algorithm's performance was evaluated under different conditions, providing valuable insights for optimizing its usage and understanding its capabilities.
![image](https://github.com/thaiqtran/EE-399-HW-6/assets/129792715/300b6ebf-d2d4-4f9b-bbed-91aa8c60ce18)
## Sec. V Conclusion
In this project, we explored the performance of an LSTM/decoder model for sea-surface temperature (SST) data analysis. Through a series of analyses, we investigated the model's performance in relation to the time lag variable, noise levels, and the number of sensors. The implemented LSTM/decoder algorithm demonstrated promising results, showcasing its effectiveness in reconstructing and forecasting SST data. The analysis of the time lag variable revealed that the model's performance was influenced by the length of historical information. The optimal time lag value was identified, providing valuable insights into capturing the temporal dependencies in the SST data accurately. Moreover, the model exhibited robustness to noise, as evidenced by its ability to handle varying levels of Gaussian noise and maintain reconstruction accuracy. This characteristic highlights the algorithm's resilience and stability in real-world scenarios with noisy data. Furthermore, the evaluation of the number of sensors demonstrated the impact of sensor configuration on the model's performance. The results shed light on the trade-off between data availability and reconstruction accuracy, aiding in the determination of the optimal sensor count for SST data analysis. Overall, the findings from this project contribute to a deeper understanding of the LSTM/decoder model's behavior and performance for SST data analysis. The results showcase its potential for climate research, weather forecasting, and environmental monitoring. By leveraging the LSTM's ability to capture temporal dependencies, the model provides valuable insights into oceanic processes, climate dynamics, and their interconnections. Moving forward, this project serves as a foundation for further exploration and optimization of LSTM-based approaches for SST data analysis. The knowledge gained from this study can inform future research and guide the development of improved models for climate and oceanographic applications. In conclusion, the LSTM/decoder model proves to be a powerful tool for SST data analysis, offering accurate reconstructions and forecasts. The project's outcomes contribute to the broader understanding of deep learning techniques in climate research, facilitating advancements in climate modeling, environmental monitoring, and data-driven decision-making processes.
